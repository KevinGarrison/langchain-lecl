{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a995edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = \"/Users/kevingarrison/Code Projects/Private Projects/Agents/Langchain_LECL/langchain-lecl\"\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5ea0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "FILE_PATH = [\"/Users/kevingarrison/Code Projects/Private Projects/Agents/Langchain_LECL/langchain-lecl/notebooks/Bachelorarbeit_Kevin_Garrison_85826.pdf\"]\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EXPORT_TYPE = ExportType.DOC_CHUNKS\n",
    "QUESTION = \"Which are the main AI models in Docling?\"\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {input}\\nAnswer:\\n\",\n",
    ")\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8953672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 12:40:24,947 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-24 12:40:24,988 - INFO - Going to convert document batch...\n",
      "2025-11-24 12:40:24,988 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-24 12:40:24,993 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-24 12:40:24,994 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-24 12:40:24,995 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-24 12:40:24,999 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-24 12:40:24,999 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-24 12:40:25,002 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-24 12:40:25,414 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-24 12:40:25,417 - INFO - Accelerator device: 'mps'\n",
      "2025-11-24 12:40:26,394 - INFO - Accelerator device: 'mps'\n",
      "2025-11-24 12:40:26,661 - INFO - Processing document Bachelorarbeit_Kevin_Garrison_85826.pdf\n",
      "2025-11-24 12:40:42,666 - INFO - Finished converting document Bachelorarbeit_Kevin_Garrison_85826.pdf in 17.72 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "loader = DoclingLoader(\n",
    "    file_path=FILE_PATH,\n",
    "    export_type=EXPORT_TYPE,\n",
    "    chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1afdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n",
    "    splits = docs\n",
    "elif EXPORT_TYPE == ExportType.MARKDOWN:\n",
    "    from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "    splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header_1\"),\n",
    "            (\"##\", \"Header_2\"),\n",
    "            (\"###\", \"Header_3\"),\n",
    "        ],\n",
    "    )\n",
    "    splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed4d67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- d.page_content='STUTTGART\\nPORSCHe\\nBachelorarbeit Studiengang : Data Science'\n",
      "- d.page_content='Einsatz von Large Language Models (LLM) zur Extraktion und Strukturierung von Zolldokumenten: Ein KI-gestützter Ansatz zur automatisierten Datenverarbeitung\\nbei Porsche AG von Kevin Garrison\\n85826\\nBetreuender Professor: Prof. Dr. Winfried Bantel Zweitprüfer : Prof. Dr. Tim Dahmen\\nEinreichungsdatum : 14. August 2025'\n",
      "- d.page_content='Angaben zur Firma\\nUnternehmen :\\nPorsche AG\\nBranche :\\nAutomobilbranche Finanzstrategie & Data Science Porscheplatz 1 D - 70435 Stuttgart\\nAbteilung :\\nAdresse :\\nBetreuerin :\\nMaike Klepsch (FOD) (+49) 0 152 3 911 0075 maike.klepsch@porsche.de\\nTelefon :\\nE-Mail :'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for d in splits[:3]:\n",
    "    print(f\"- {d.page_content=}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf56e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 327 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split blog post into {len(splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fffa06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 12:40:46,671 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eb73401a-b3e3-45fd-b15e-512a157a923e', 'fd7d7638-5a8d-4efb-80bf-81dea9cf6566', '9f16d1eb-9e48-4c27-b0bb-f76a04f8ef81']\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45daf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=TOP_K)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff8847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")\n",
    "\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a6e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the key findings of the paper?\n",
      "\n",
      "Once you get the answer, retrive the .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 12:40:49,168 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_d8nqZBzwrZ3aZgUFlBwR3Pbk)\n",
      " Call ID: call_d8nqZBzwrZ3aZgUFlBwR3Pbk\n",
      "  Args:\n",
      "    query: key findings of the paper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 12:40:49,459 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': '/Users/kevingarrison/Code Projects/Private Projects/Agents/Langchain_LECL/langchain-lecl/notebooks/Bachelorarbeit_Kevin_Garrison_85826.pdf', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/27', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 88.931, 't': 642.6360146484375, 'r': 507.798, 'b': 511.0610146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 750]}]}], 'headings': ['Kurzfassung'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 18027861618462669308, 'filename': 'Bachelorarbeit_Kevin_Garrison_85826.pdf'}}}\n",
      "Content: Kurzfassung\n",
      "Ergebnisse hinzunehmen.\n",
      "\n",
      "Source: {'source': '/Users/kevingarrison/Code Projects/Private Projects/Agents/Langchain_LECL/langchain-lecl/notebooks/Bachelorarbeit_Kevin_Garrison_85826.pdf', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/28', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 5, 'bbox': {'l': 88.899, 't': 507.1440146484375, 'r': 508.114, 'b': 199.4290146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 1764]}]}], 'headings': ['Kurzfassung'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 18027861618462669308, 'filename': 'Bachelorarbeit_Kevin_Garrison_85826.pdf'}}}\n",
      "Content: Kurzfassung\n",
      "klassischen Systems erheblich beeinträchtigen, während der LLM-basierte Ansatz diese Schwächen durch kontextuelle Interpretation teilweise kompensieren kann. Die besten Resultate hinsichtlich der Micro-Metriken lagen bei einem Recall von 0,53 und einem F1-Score von 0,60. Diese Ergebnisse wurden bei exakter Übereinstimmung numerischer Werte und einem Levenshtein-Schwellenwert von über 90% im Zeichenkettenvergleich zu den Testdaten erzielt. Insgesamt zeigen die Ergebnisse, dass der LLM-Ansatz robuster gegenüber fehlerhaften Eingangsdaten ist und ein höheres Maß an Generalisierungsfähigkeit aufweist, zugleich jedoch auch eigene Limitationen in Bezug auf Reproduzierbarkeit und Extraktionsgenauigkeit mit sich bringt, sodass eine\n",
      "\n",
      "Source: {'source': '/Users/kevingarrison/Code Projects/Private Projects/Agents/Langchain_LECL/langchain-lecl/notebooks/Bachelorarbeit_Kevin_Garrison_85826.pdf', 'dl_meta': {'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/517', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 55, 'bbox': {'l': 88.931, 't': 454.64801464843754, 'r': 507.799, 'b': 323.0730146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 733]}]}, {'self_ref': '#/texts/518', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 55, 'bbox': {'l': 88.866, 't': 319.15701464843755, 'r': 507.797, 'b': 255.3270146484375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 348]}]}], 'headings': ['6.1. Erreichte Ergebnisse'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 18027861618462669308, 'filename': 'Bachelorarbeit_Kevin_Garrison_85826.pdf'}}}\n",
      "Content: 6.1. Erreichte Ergebnisse\n",
      "Schwellenwerten größer 90% erzielt.\n",
      "Die Ergebnisse verdeutlichen zudem, dass die Leistungsfähigkeit des Systems maßgeblich von der Qualität und Struktur der zugrunde liegenden Daten beeinflusst wird. Insgesamt zeigen die empirischen Analysen, dass die Extraktionspipeline derzeit noch nicht ausreichend robust ist, um zollkritische Prozesse vollumfänglich und automatisiert abzulösen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 12:40:53,360 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the key findings of the paper:\n",
      "\n",
      "- The classic system's performance is significantly impaired by erroneous input data. In contrast, the LLM-based (Large Language Model) approach can partially compensate for these weaknesses through contextual interpretation.\n",
      "- The best results with the LLM approach were achieved with a recall of 0.53 and an F1-score of 0.60, especially when there was exact numeric value matching combined with a Levenshtein threshold above 90% for string comparisons to test data.\n",
      "- The results indicate that the LLM approach is more robust to faulty input data and demonstrates a higher degree of generalization capability. However, it also has its own limitations regarding reproducibility and extraction precision.\n",
      "- The system's performance heavily depends on the quality and structure of the underlying data.\n",
      "- The empirical analysis reveals that the current extraction pipeline is still not robust enough to completely and automatically replace critical customs processes.\n",
      "\n",
      "If you need further details or the source text, let me know!\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What are the key findings of the paper?\\n\\n\"\n",
    "    \"Once you get the answer, retrive the .\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-lecl (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
